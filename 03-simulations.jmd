---
title : ODE parameter estimation
author : Andrew Irwin
date: 2021-04-20
weave_options:
  # doctype: github
  out_width: "75%"
  dpi: 144
  wrap: false
---

# Parameter estimation from simulated data

## Packages

```julia, results = "hidden"
using DifferentialEquations, Plots
using Flux, DiffEqFlux, Optim, DiffEqSensitivity
import Statistics
using Turing, Distributions, DifferentialEquations 
using MCMCChains, StatsPlots
using Random
using Logging
```

## Simulating data

I start with the Droop model. 


```julia, results = "hidden"
function droop!(du, u, p, t)
  R, Q, X = u
  Km, Vmax, Qmin, muMax, d, R0 = p
  rho = Vmax * R / (Km + R)
  mu = muMax * (1 - Qmin/Q)
  du[1] = dRdt = d*(R0 - R) - rho*X
  du[2] = dQdt = rho - mu*Q
  du[3] = dXdt = (mu - d)*X
end
```

The initial conditions, parameters, and time-span for the solution must be specified.
This time we specify some times to sample the solution to obtain data to use in parameter estimation.

```julia, results = "hidden"
u0 = [1.0, 1.0, 1.0]
p = [0.1, 2.0, 1.0, 0.8, 0.0, 0.0]
tspan = (0.0, 10.0)
tsteps = [0.1, 3.2, 4.5, 7.0, 9.1]
```

Now we create and solve the ODE initial value problem. We will create two solutions,
one smoothly estimating the solution with interpolation on an interval and one with 
samples at a few discrete points.


```julia, results = "hidden"
prob = ODEProblem(droop!, u0, tspan, p)
sol1 = solve(prob, Tsit5())
sol2 = solve(prob, Rosenbrock23(), saveat = tsteps)
```

This plot shows the solution and the discrete samples.


```julia, echo=false
Plots.plot(sol1)
Plots.scatter!(sol2)
```

We can convert the solution to a matrix and add some noise to the output. These data will be used to define the objective function in the optimization.

```julia
data1 = Array(sol2) 
data1 = data1 .* (1 .+ 0.05*randn(size(data1)));
Plots.plot(sol1, label = ['R' 'Q' 'X'])
Plots.scatter!(sol2.t, data1', label = ['R' 'Q' 'X'])
```


## Optimization to find parameters

Now write a function to describe the difference between a trial solution and the
data points.  For the optimizer we use next, this should be a function of just the 
parameters to be adjusted.
Here my loss function is the sum of squared difference between solution and data. I experimented with scaling by standard deviation 
of each variable in data, but ran into trouble with R which is usually 0.

```julia, results = "hidden"
function loss(p)
    tspan = (0.0, 10.0)
    u0 = p[1:3]
    param = [ p[4:7] ; 0.0 ; 0.0 ] # force d = Rin = 0.0
    prob = ODEProblem(droop!, u0, tspan, param)
    sol2 = solve(prob, Rosenbrock23(), saveat = tsteps)
    data2 = Array(sol2)
    # loss = sum(((1.0 ./ Statistics.std(data1, dims=2)') * (data2 .- data1) ) .^ 2 )
    # loss = sum(((1.0 ./ (1 .+ Statistics.std(data1, dims=2)')) * (data2 .- data1) ) .^ 2 )
    loss = sum( (data2 .- data1) .^ 2 )
  return loss , sol2
end
```

(Note: I originally standardized the three variables as in the commented loss definition above, but 
this resulted in a worse solution, because R was so close to 0 and has a standard deviation of $10^{-7}$. Perhaps a solution is to add a small amount to standard deviations to prevent this distortion. My first 
attempt, also commented out, led to instabilities.)

The optimizer allows us to provide a callback function to show the loss score, or make a plot, at each iteration.  Here's an example callback function.
The callback has access to all the return values of the loss function.

```julia, results = "hidden"
callback = function (p, l, pred)
  display(l)
  # plt = plot(pred, ylim = (0, 6))
  # display(plt)
  # Tell sciml_train to not halt the optimization. If return true, then
  # optimization stops.
  return false
end
```

Test the loss function before proceeding.

```julia
loss([1.0; 1.0; 1.0; p])
```

We are now ready to use the optimizer. ADAM is the gradient-search method. The optimizer is defined in [DiffEqFlux](https://diffeqflux.sciml.ai/v1.34/Scimltrain/) but it can use many different backends.
The optimizer is [ADAM](https://arxiv.org/abs/1412.6980)


```julia, results = "hidden"
result_ode = DiffEqFlux.sciml_train(loss, [1.0, 1.0, 1.0, 0.1, 2.0, 1.0, 1.0, 0, 0],
                                    ADAM(0.001),  # this argument needs to be small or instability
                                    cb = callback,
                                    maxiters = 200)
```

Now we use the parameters from this optimization to solve the differential equation
and plot the solution.

```julia, results = "hidden"
prob = ODEProblem(droop!, result_ode.u[1:3], tspan, result_ode.u[4:end])
sol3 = solve(prob, Rosenbrock23())
```

We will plot the solution and the discrete samples (thin lines for original solution,
thick lines for solution estimated from data.)

```julia, echo=false
Plots.plot(sol1, label = ["Original R" 'Q' 'X'])
Plots.plot!(sol3, lw = 2, label = ["New R" 'Q' 'X'])
Plots.scatter!(sol2.t, data1', label = ["Data R" 'Q' 'X'])
```

Here we compare parameters; agreement is excellent (a few percent error) except
for Km.

```julia, echo=false
xaxis = ["R0" "Q0" "X0" "Km" "Vmax" "Qmin" "muMax" ]
estimate = result_ode.u[1:(end-2)]'
original = [u0; p][1:(end-2)]'
error = (estimate .- original ) ./ original
p1 = Plots.scatter(xaxis, estimate, legend = false, title = "Parameters")
p1 = Plots.scatter!(xaxis, original, legend = false)
p2 = Plots.scatter(xaxis[1:7], error[1:7], legend = false, title = "Relative error")
plot(p1, p2, layout = (2,1))
```

## Bayesian MCMC parameter estimation

Now we will solve the same problem using a completely different approach. We will get a distribution
on each parameter and plot many sets of solutions to illustrate the consequences of
the uncertainty on the trajectories.

Set a random seed to make the result reproducible. Select an option for Turing package.

```julia, results = "hidden"
Random.seed!(14)
Turing.setadbackend(:forwarddiff)
```

Define a model for the parameters, including priors, solution of the ODE, and comparison between data and the solution.

```julia, results = "hidden"
@model function fitDroop1(t, R, Q, X)
    σ1 ~ InverseGamma(2, 3)  # positive support; parameters α, β; mean β/(α-1), here 3
    σ2 ~ InverseGamma(2, 3) 
    σ3 ~ InverseGamma(2, 3) 
    R0 ~ truncated(Normal(1, 1), 0, 5) # might need strictly positive values?
    Q0 ~ truncated(Normal(1, 1), 0, 5)
    X0 ~ truncated(Normal(1, 1), 0, 5)
    Km ~ truncated(Normal(4.0, 2), 0, 5)
    Vmax ~ truncated(Normal(1.2, 2), 0, 5)
    Qmin ~ truncated(Normal(1.0, 2), 0, 5)
    muMax ~ truncated(Normal(1.0, 2), 0, 5)

    p = [ Km, Vmax, Qmin, muMax, 0.0, 0.0]

    # must define the problem with numeric values first, then update with distributions
    prob1 = ODEProblem(droop!, [R[1], Q[1], X[1]], (0.0, 10.0), [0.1, 1.0, 1.0, 1.0, 0.0, 0.0])
    prob = remake(prob1, u0=[R0, Q0, X0], p=p)  
    predicted = solve(prob, Rosenbrock23(), saveat=t)
    
    for j = 1:length(t)
        Q[j] ~ Normal(predicted[j][2], σ1)
        X[j] ~ Normal(predicted[j][3], σ2)
    end
    R[1] ~ Normal(predicted[1][1], σ3)   # Just use the first (non-zero) DIN to anchor the solution
end
```

Create the model and simulate the chains.

```julia, results = "hidden"
t = sol2.t
R = [v[1] for v in sol2.u]
Q = [v[2] for v in sol2.u]
X = [v[3] for v in sol2.u]

model = fitDroop1(t, R, Q, X)
chain2 = sample(model, NUTS(.65), MCMCThreads(), 2000, 4, progress=false)  # multi-threaded
# chain2 = mapreduce(c -> sample(model, NUTS(.75), 200), chainscat, 1:4) # single thread
```

Show the results in tabular form and as traceplots and distributions.

```julia, term=true
chain2
plot(chain2)
```

Extract data so we can plot trajectories from a selection of parameter values from the posterior distribution. Parameters come out of the chains in alphabetical order, so I resequence them to be in the order: initial conditons, parameter values as used in ODE function.

```julia, results = "hidden"
chain_array0 = Array(chain2);
chain_array = chain_array0[: ,[4, 2, 6, 1, 5, 3, 7] ]  # R Q X Km Vmax Qmin muMax
p_median = [ median(chain_array, dims=1)  0.0  0.0]
sol3 = solve(remake(prob, u0 = p_median[1:3], p = p_median[4:end]), Rosenbrock23())
```

Show the original solution (thin, coloured line), estimated solution using median of the posterior distribution
for each parameter (thick, coloured line), and sample of 300 trajectories from the posterior distribution samples (gray lines).


```julia
N = size(chain_array)[1]
N2 = Int(N/2)
Plots.scatter(t, R);
for k in 1:300
    pars = [ chain_array[rand(N2:N), :];  0.0; 0.0 ] # append d, Rin
    resol = solve(remake(prob, u0 = pars[1:3], p = pars[4:end]), Rosenbrock23()) 
    plot!(resol, vars=(0,1), alpha=0.3, color = "#BBBBBB", legend = false)
end
plot!(sol3, vars=(0,1), alpha=1, color = "#BB0000", legend = false, lw=2)
pR = plot!(sol1, vars=(0,1), alpha=1, color = "#BB0000", legend = false, ylims=(0, Inf),
        yguide = "R")
pR = Plots.scatter!(t, R);
Plots.scatter(t, Q);
for k in 1:300
    pars = [ chain_array[rand(N2:N), :];  0.0; 0.0 ] # append d, Rin
    resol = solve(remake(prob, u0 = pars[1:3], p = pars[4:end]), Rosenbrock23()) 
    plot!(resol, vars=(0,2), alpha=0.3, color = "#BBBBBB", legend = false)
end
plot!(sol3, vars=(0,2), alpha=1, color = "#BB0000", legend = false, lw=2)
pQ = plot!(sol1, vars=(0,2), alpha=1, color = "#BB0000", legend = false, ylims=(0, Inf),
        yguide = "Q")
pQ = Plots.scatter!(t, Q);
Plots.scatter(t, log.(X));
for k in 1:300
    pars = [ chain_array[rand(N2:N), :];  0.0; 0.0 ] # append d, Rin
    resol = solve(remake(prob, u0 = pars[1:3], p = pars[4:end]), Rosenbrock23()) 
    plot!(resol, vars=((t,x) -> (t, log.(x)), 0, 3), alpha=0.3, color = "#BBBBBB", legend = false)
end
plot!(sol3, vars=((t,x) -> (t, log.(x)), 0,3), alpha=1, color = "#BB0000", legend = false, lw=2)
pX = plot!(sol1, vars=((t,x) -> (t, log.(x)), 0,3), alpha=1, color = "#BB0000", legend = false,
        yguide = "log X")
pX = Plots.scatter!(t, log.(X));
plot(pR, pQ, pX, layout = (3,1))
```

It looks like having only a few data points leaves a lot of uncertainty about the trajectory!

